{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJCMTixS_Gl8"
      },
      "source": [
        "**Data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8LlRMjpqwpjE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/topical_chat.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "D6cliv2myFuR",
        "outputId": "8998d5f5-d16e-437b-deea-623010567d86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cbdf7df0-b661-45a6-871d-03d83236b4dd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>message</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Are you a fan of Google or Microsoft?</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Both are excellent technology they are helpfu...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I'm not  a huge fan of Google, but I use it a...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Google provides online related services and p...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Yeah, their services are good. I'm just not a...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188373</th>\n",
              "      <td>8628</td>\n",
              "      <td>Wow, it does not seem like that long. Since I...</td>\n",
              "      <td>Surprised</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188374</th>\n",
              "      <td>8628</td>\n",
              "      <td>I havent seen that episode, I might google it...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188375</th>\n",
              "      <td>8628</td>\n",
              "      <td>I don't think I have either. That's an insane...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188376</th>\n",
              "      <td>8628</td>\n",
              "      <td>I did, my little brother used to love Thomas ...</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188377</th>\n",
              "      <td>8628</td>\n",
              "      <td>It did. Ringo Starr, George Carlin, and Alec ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188378 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbdf7df0-b661-45a6-871d-03d83236b4dd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cbdf7df0-b661-45a6-871d-03d83236b4dd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cbdf7df0-b661-45a6-871d-03d83236b4dd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d55557dc-dc97-4a77-a9b6-57c852f8289e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d55557dc-dc97-4a77-a9b6-57c852f8289e')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d55557dc-dc97-4a77-a9b6-57c852f8289e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        conversation_id                                            message  \\\n",
              "0                     1              Are you a fan of Google or Microsoft?   \n",
              "1                     1   Both are excellent technology they are helpfu...   \n",
              "2                     1   I'm not  a huge fan of Google, but I use it a...   \n",
              "3                     1   Google provides online related services and p...   \n",
              "4                     1   Yeah, their services are good. I'm just not a...   \n",
              "...                 ...                                                ...   \n",
              "188373             8628   Wow, it does not seem like that long. Since I...   \n",
              "188374             8628   I havent seen that episode, I might google it...   \n",
              "188375             8628   I don't think I have either. That's an insane...   \n",
              "188376             8628   I did, my little brother used to love Thomas ...   \n",
              "188377             8628   It did. Ringo Starr, George Carlin, and Alec ...   \n",
              "\n",
              "                      sentiment  \n",
              "0        Curious to dive deeper  \n",
              "1        Curious to dive deeper  \n",
              "2        Curious to dive deeper  \n",
              "3        Curious to dive deeper  \n",
              "4        Curious to dive deeper  \n",
              "...                         ...  \n",
              "188373                Surprised  \n",
              "188374   Curious to dive deeper  \n",
              "188375   Curious to dive deeper  \n",
              "188376                    Happy  \n",
              "188377                  Neutral  \n",
              "\n",
              "[188378 rows x 3 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THNKp1Ybg1Eo",
        "outputId": "67def40c-e8b6-45d7-a27c-67c4cf43ae33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation 1:\n",
            "Question:  Are you a fan of Google or Microsoft?\n",
            "Answer:  Both are excellent technology they are helpful in many ways. For the security purpose both are super.\n",
            "\n",
            "Conversation 2:\n",
            "Question:  I'm not  a huge fan of Google, but I use it a lot because I have to. I think they are a monopoly in some sense. \n",
            "Answer:  Google provides online related services and products, which includes online ads, search engine and cloud computing.\n",
            "\n",
            "Conversation 3:\n",
            "Question:  Yeah, their services are good. I'm just not a fan of intrusive they can be on our personal lives. \n",
            "Answer:  Google is leading the alphabet subsidiary and will continue to be the Umbrella company for Alphabet internet interest.\n",
            "\n",
            "Conversation 4:\n",
            "Question:  Did you know Google had hundreds of live goats to cut the grass in the past? \n",
            "Answer:  It is very interesting. Google provide \"Chrome OS\" which is a light weight OS. Google provided a lot of hardware mainly in 2010 to 2015. \n",
            "\n",
            "Conversation 5:\n",
            "Question:  I like Google Chrome. Do you use it as well for your browser? \n",
            "Answer:  Yes.Google is the biggest search engine and Google service figure out top 100 website, including Youtube and Blogger.\n",
            "\n",
            "Conversation 6:\n",
            "Question:  By the way, do you like Fish? \n",
            "Answer:  Yes. They form a sister group of tourniquets- they make the sea water clean and remove the dust from it. Fish is the biggest part in the eco-system.\n",
            "\n",
            "Conversation 7:\n",
            "Question:  Did you know that a seahorse is the only fish to have a neck? \n",
            "Answer:  Freshwater fish only drink water through the skin via Osmosis, Saltwater fish drink water through the mouth. Dolphins are friendly to human beings.\n",
            "\n",
            "Conversation 8:\n",
            "Question:  Interesting, they also have gills. Did you know that jellyfish are immortal? \n",
            "Answer:  Yes. Fish is the important resources of human world wide for the commercial and subsistence fish hunts the fish in the wild fisheries.\n",
            "\n",
            "Conversation 9:\n",
            "Question:  What about cats, do you like cats? I'm a dog fan myself. \n",
            "Answer:  The cat is referred as domestic cat and wild cat. They make our world very clean from rats! \n",
            "\n",
            "Conversation 10:\n",
            "Question:  Yeah, cats can be cool, but they sure do spend a lot of their time sleeping. \n",
            "Answer:  Cats hear the sounds too faint or too high frequency human ears can hear. \n",
            "\n",
            "Conversation 11:\n",
            "Question:  do you like dance?\n",
            "Answer:  Yes  I do. Did you know Bruce Lee was a cha cha dancer?\n",
            "\n",
            "Conversation 12:\n",
            "Question:  Yes he even won a hardcore cha cha championship in 1958\n",
            "Answer:  Yeah. Did you know Tupac was a ballet dancer?\n",
            "\n",
            "Conversation 13:\n",
            "Question:  Yes and he even was in the production of the nutcracker\n",
            "Answer:  Yeah. Ballet dancer go through 4 pairs of shoes a week\n",
            "\n",
            "Conversation 14:\n",
            "Question:  Yes that is a lot of shoes and also a lot of money\n",
            "Answer:  Yeah true. Did you know babies are really good at dancing?\n",
            "\n",
            "Conversation 15:\n",
            "Question:  Yes and they smile more when they hit the beat\n",
            "Answer:  Yeah they are much smarter than we give them credit for\n",
            "\n",
            "Conversation 16:\n",
            "Question:  True Did you know Jackson had a patent on a dancing device?\n",
            "Answer:  Yes it helped him smooth out his dance moves\n",
            "\n",
            "Conversation 17:\n",
            "Question:  Nice. Do you like Shakespeare?\n",
            "Answer:  Yes I do. Do you know that he popularized many phrases\n",
            "\n",
            "Conversation 18:\n",
            "Question:  Yes like good riddance, in my heart of hearts and such\n",
            "Answer:  Yes and then he also invented names like Jessica, Olivia and Miranda\n",
            "\n",
            "Conversation 19:\n",
            "Question:  Yes. And for his works you have to use old english for it to make sense\n",
            "Answer:  Yes otherwise the rhymes and puns do not seem to work out\n",
            "\n",
            "Conversation 20:\n",
            "Question:  Yes. He lived at the same time as Pocahontas too\n",
            "Answer:  I wonder if they met how that would go from there\n",
            "\n",
            "Conversation 21:\n",
            "Question:  Hey what's up do use Google very often?I really love the company and was surprised to hear that it was founded back in 1998.\n",
            "Answer:  i think everyone must use it daily! its become ingrained in every day life\n",
            "\n",
            "Conversation 22:\n",
            "Question:  Agreed. The Google headquarters in Mountain View California is nicknamed the Google Plex.\n",
            "Answer:  thats funny. The current CEO is Sundar Pichai, i didnt know Larry Page was replaced\n",
            "\n",
            "Conversation 23:\n",
            "Question:  Oh yeah I didn't know that either. I also want to go to google Plex to see the goats who mow their lawn by eating it.\n",
            "Answer:  say what now?? they have that??\n",
            "\n",
            "Conversation 24:\n",
            "Question:  Yeah apparently lol! They do that instead of hiring people to mow!\n",
            "Answer:  thats both funny and i guess imaginative. leave it to a huge tech company to employ actual goats!\n",
            "\n",
            "Conversation 25:\n",
            "Question:  Yeah exactly I am sure they are cheaper. One thing I bet they couldn't exploit is fish. I think fish are so cool there is actually a breed of jellyfish that is immortal.\n",
            "Answer:  i had rememered hearing about that before. Immortatlity is wasted on a jellyfish haha. did you know a seahorse is the only fish that has an actual neck?\n",
            "\n",
            "Conversation 26:\n",
            "Question:  That is so funny I guess I never considered a seahorse a fish. The black swallower fish sounds a lot like a snake because it can eat pray that is so large.\n",
            "Answer:  i guess they live up to their name then!\n",
            "\n",
            "Conversation 27:\n",
            "Question:  It seems they do. I also didn't know that there was a difference with how freshwater and saltwater fish drink.\n",
            "Answer:  thats crazy. i wonder why fresh water ones only use osmosis? \n",
            "\n",
            "Conversation 28:\n",
            "Question:  Yeah and saltwater fish are lucky because they can do that and drink through their mouth's.\n",
            "Answer:  seems like fresh water fish got the short end of the stick with that one. Have you ever been to a cat cafe?\n",
            "\n",
            "Conversation 29:\n",
            "Question:  I have never been to a cat cafe no, what about you? Seems like they are popular in Japan and Taiwan.\n",
            "Answer:  no but I would love to! paying hourly to hang out with adorable cats? im in!\n",
            "\n",
            "Conversation 30:\n",
            "Question:  Yeah that would be a lot of fun. I didn't realize that cats sleep so much. Must be nice.\n",
            "Answer:  i guess thats where the phrase \"cat nap\" comes from\n",
            "\n",
            "Conversation 31:\n",
            "Question:  Hi!  do you like to dance?\n",
            "Answer:  I love to dance a lot. How about you?\n",
            "\n",
            "Conversation 32:\n",
            "Question:  I am really bad, but it is a good time.\n",
            "Answer:  Dancing is a lot of fun. Did you know that Bruce Lee was a great dancer?\n",
            "\n",
            "Conversation 33:\n",
            "Question:  I heard that, winning Cha Cha championships and everything!\n",
            "Answer:  Yes that is amazing. He won the Hong Kong cha-cha championship back in 1958 in fact.\n",
            "\n",
            "Conversation 34:\n",
            "Question:  I always just thought of him as a martial arts legend.  Now he is a dance legend of sorts too!\n",
            "Answer:  Yeah!! That is correct. He was a fantastic martial artist. Did you know that Tupac danced ballet in high school?\n",
            "\n",
            "Conversation 35:\n",
            "Question:  Yeah!  He was the mouse king in the Nutcracker.  Thats pretty cool, I would definitely never have guessed that about him.\n",
            "Answer:  Neither did I. That is insane because Tupac was a famous rapper. \n",
            "\n",
            "Conversation 36:\n",
            "Question:  He was indeed, his music is even in the library of congress.\n",
            "Answer:  I didn't know this thanks for sharing.\n",
            "\n",
            "Conversation 37:\n",
            "Question:  Sure thing!  Did you hear about Michael Jackson's special patent shoes?\n",
            "Answer:  No. I know that Michael Jackson was a fantastic dancer but can you tell me more about his patent shoes if you don't mind.\n",
            "\n",
            "Conversation 38:\n",
            "Question:  There was a specific device in his shoes that helped with his extreme lean in some dance moves.\n",
            "Answer:  Wow!!! That is amazing coming from such a talented singer and dancer. I couldn't even dance like that even if I dreamed of it.\n",
            "\n",
            "Conversation 39:\n",
            "Question:  Me neither. I could never be a professional dancer.\n",
            "Answer:  I heard that some professional ballet dancer can go through four pairs of shoes in a week.\n",
            "\n",
            "Conversation 40:\n",
            "Question:  That is crazy!  That can't be cheap for them.\n",
            "Answer:  No. I think its very expensive for them to be professional ballet dancers based on this.\n",
            "\n",
            "Conversation 41:\n",
            "Question:  It has been great chatting but I must go!  Gotta go get my Bruce Lee on, the martial arts part...  definitely not the dance.\n",
            "Answer:  Ha Ha!!! It was so nice chatting with you as well!! Have a nice day!!! Bye\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Group messages by conversation ID\n",
        "grouped_data = data.groupby('conversation_id')['message'].apply(list)\n",
        "\n",
        "# Prepare pairs of questions and answers for each conversation\n",
        "conversations = []\n",
        "for _, messages in grouped_data.items():\n",
        "    questions = messages[::2]  # Assuming questions are at even indices\n",
        "    answers = messages[1::2]   # Assuming answers are at odd indices\n",
        "\n",
        "    for question, answer in zip(questions, answers):\n",
        "        conversations.append({'question': question, 'answer': answer})\n",
        "\n",
        "# Print the first few conversations for verification\n",
        "for i, conv in enumerate(conversations):\n",
        "    print(f\"Conversation {i + 1}:\")\n",
        "    print(f\"Question: {conv['question']}\")\n",
        "    print(f\"Answer: {conv['answer']}\\n\")\n",
        "    if i >= 40:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otcl5bsL3OOn",
        "outputId": "5e0f5000-26e5-4319-efb7-8fd1106932ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation 1:\n",
            "Cleaned Question: are you a fan of google or microsoft\n",
            "Cleaned Answer: both are excellent technology they are helpful in many ways for the security purpose both are super\n",
            "\n",
            "Conversation 2:\n",
            "Cleaned Question: im not a huge fan of google but i use it a lot because i have to i think they are a monopoly in some sense\n",
            "Cleaned Answer: google provides online related services and products which includes online ads search engine and cloud computing\n",
            "\n",
            "Conversation 3:\n",
            "Cleaned Question: yeah their services are good im just not a fan of intrusive they can be on our personal lives\n",
            "Cleaned Answer: google is leading the alphabet subsidiary and will continue to be the umbrella company for alphabet internet interest\n",
            "\n",
            "Conversation 4:\n",
            "Cleaned Question: did you know google had hundreds of live goats to cut the grass in the past\n",
            "Cleaned Answer: it is very interesting google provide chrome os which is a light weight os google provided a lot of hardware mainly in 2010 to 2015\n",
            "\n",
            "Conversation 5:\n",
            "Cleaned Question: i like google chrome do you use it as well for your browser\n",
            "Cleaned Answer: yesgoogle is the biggest search engine and google service figure out top 100 website including youtube and blogger\n",
            "\n",
            "Conversation 6:\n",
            "Cleaned Question: by the way do you like fish\n",
            "Cleaned Answer: yes they form a sister group of tourniquets they make the sea water clean and remove the dust from it fish is the biggest part in the ecosystem\n",
            "\n",
            "Conversation 7:\n",
            "Cleaned Question: did you know that a seahorse is the only fish to have a neck\n",
            "Cleaned Answer: freshwater fish only drink water through the skin via osmosis saltwater fish drink water through the mouth dolphins are friendly to human beings\n",
            "\n",
            "Conversation 8:\n",
            "Cleaned Question: interesting they also have gills did you know that jellyfish are immortal\n",
            "Cleaned Answer: yes fish is the important resources of human world wide for the commercial and subsistence fish hunts the fish in the wild fisheries\n",
            "\n",
            "Conversation 9:\n",
            "Cleaned Question: what about cats do you like cats im a dog fan myself\n",
            "Cleaned Answer: the cat is referred as domestic cat and wild cat they make our world very clean from rats\n",
            "\n",
            "Conversation 10:\n",
            "Cleaned Question: yeah cats can be cool but they sure do spend a lot of their time sleeping\n",
            "Cleaned Answer: cats hear the sounds too faint or too high frequency human ears can hear\n",
            "\n",
            "Conversation 11:\n",
            "Cleaned Question: do you like dance\n",
            "Cleaned Answer: yes i do did you know bruce lee was a cha cha dancer\n",
            "\n",
            "Conversation 12:\n",
            "Cleaned Question: yes he even won a hardcore cha cha championship in 1958\n",
            "Cleaned Answer: yeah did you know tupac was a ballet dancer\n",
            "\n",
            "Conversation 13:\n",
            "Cleaned Question: yes and he even was in the production of the nutcracker\n",
            "Cleaned Answer: yeah ballet dancer go through 4 pairs of shoes a week\n",
            "\n",
            "Conversation 14:\n",
            "Cleaned Question: yes that is a lot of shoes and also a lot of money\n",
            "Cleaned Answer: yeah true did you know babies are really good at dancing\n",
            "\n",
            "Conversation 15:\n",
            "Cleaned Question: yes and they smile more when they hit the beat\n",
            "Cleaned Answer: yeah they are much smarter than we give them credit for\n",
            "\n",
            "Conversation 16:\n",
            "Cleaned Question: true did you know jackson had a patent on a dancing device\n",
            "Cleaned Answer: yes it helped him smooth out his dance moves\n",
            "\n",
            "Conversation 17:\n",
            "Cleaned Question: nice do you like shakespeare\n",
            "Cleaned Answer: yes i do do you know that he popularized many phrases\n",
            "\n",
            "Conversation 18:\n",
            "Cleaned Question: yes like good riddance in my heart of hearts and such\n",
            "Cleaned Answer: yes and then he also invented names like jessica olivia and miranda\n",
            "\n",
            "Conversation 19:\n",
            "Cleaned Question: yes and for his works you have to use old english for it to make sense\n",
            "Cleaned Answer: yes otherwise the rhymes and puns do not seem to work out\n",
            "\n",
            "Conversation 20:\n",
            "Cleaned Question: yes he lived at the same time as pocahontas too\n",
            "Cleaned Answer: i wonder if they met how that would go from there\n",
            "\n",
            "Conversation 21:\n",
            "Cleaned Question: hey whats up do use google very ofteni really love the company and was surprised to hear that it was founded back in 1998\n",
            "Cleaned Answer: i think everyone must use it daily its become ingrained in every day life\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download necessary resources if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define a function to clean and preprocess the text\n",
        "def clean_and_preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = remove_punctuation(text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Join tokens back into a cleaned sentence\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    # Remove punctuation using the translation table\n",
        "    text_without_punctuation = text.translate(translator)\n",
        "\n",
        "    return text_without_punctuation\n",
        "\n",
        "# Clean and preprocess the questions and answers in conversations\n",
        "cleaned_conversations = []\n",
        "for conv in conversations:\n",
        "    cleaned_question = clean_and_preprocess(conv['question'])\n",
        "    cleaned_answer = clean_and_preprocess(conv['answer'])\n",
        "    cleaned_conversations.append({'question': cleaned_question, 'answer': cleaned_answer})\n",
        "\n",
        "# Print the first few cleaned conversations for verification\n",
        "for i, conv in enumerate(cleaned_conversations):\n",
        "    print(f\"Conversation {i + 1}:\")\n",
        "    print(f\"Cleaned Question: {conv['question']}\")\n",
        "    print(f\"Cleaned Answer: {conv['answer']}\\n\")\n",
        "    if i >= 20:  # Print the first 5 cleaned conversations\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRxwwK7c7Z8r",
        "outputId": "7801adcd-44df-4274-ecb1-8542848eb021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 41913\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Combine all cleaned questions and answers into a single list\n",
        "all_text = [conv['question'] + ' ' + conv['answer'] for conv in cleaned_conversations]\n",
        "\n",
        "# Initialize a Tokenizer\n",
        "tokenizer = Tokenizer()  # Use <OOV> for out-of-vocabulary words\n",
        "\n",
        "# Fit the tokenizer on the text\n",
        "tokenizer.fit_on_texts(all_text)\n",
        "\n",
        "# Add '<start>' and '<end>' tokens to the tokenizer's word_index\n",
        "tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
        "tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 2\n",
        "\n",
        "# Convert text to sequences of word indices\n",
        "sequences = tokenizer.texts_to_sequences(all_text)\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_seq_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "# Pad sequences to make them of the same length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post', truncating='post')\n",
        "\n",
        "# Create input-output pairs for the encoder-decoder model\n",
        "input_data = padded_sequences[:, :-1]  # Input is the question (remove the last token)\n",
        "output_data = padded_sequences[:, 1:]   # Output is the answer (remove the first token)\n",
        "\n",
        "# Convert input and output sequences to numpy arrays\n",
        "input_data = np.array(input_data)\n",
        "output_data = np.array(output_data)\n",
        "\n",
        "# Print the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index)\n",
        "print(f\"Vocabulary size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUgEXsOX1n-W",
        "outputId": "605adce4-acf5-46f3-cffb-62e6ae904d6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "187"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_seq_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgL_sFnf2TXB",
        "outputId": "86fcebc1-b369-4045-a749-2495e1b6f862"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "187"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(padded_sequences)\n",
        "pad_max_seq_length = max(len(seq) for seq in padded_sequences)\n",
        "pad_max_seq_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPezk2gY2Azs",
        "outputId": "6429219f-56f2-4445-adf9-a709d65379de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "91174"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haYZzVSm48hy",
        "outputId": "5f30418d-b42a-4033-fb38-52afc963ae58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1520/1520 [==============================] - 581s 372ms/step - loss: inf - accuracy: 0.8570 - val_loss: 0.4443 - val_accuracy: 0.9449\n",
            "570/570 [==============================] - 79s 138ms/step - loss: 0.4443 - accuracy: 0.9449\n",
            "Test Loss: 0.4443\n",
            "Test Accuracy: 0.9449\n",
            "Perplexity: 1.5593\n",
            "BLEU Score: 0.0000\n",
            "Recall: 0.0714\n",
            "Epoch 2/5\n",
            "1520/1520 [==============================] - 495s 326ms/step - loss: 0.2631 - accuracy: 0.9688 - val_loss: 0.1524 - val_accuracy: 0.9843\n",
            "570/570 [==============================] - 80s 140ms/step - loss: 0.1524 - accuracy: 0.9843\n",
            "Test Loss: 0.1524\n",
            "Test Accuracy: 0.9843\n",
            "Perplexity: 1.1647\n",
            "BLEU Score: 0.0000\n",
            "Recall: 0.0714\n",
            "Epoch 3/5\n",
            "1520/1520 [==============================] - 489s 322ms/step - loss: 0.1008 - accuracy: 0.9898 - val_loss: 0.0727 - val_accuracy: 0.9931\n",
            "570/570 [==============================] - 78s 137ms/step - loss: 0.0727 - accuracy: 0.9931\n",
            "Test Loss: 0.0727\n",
            "Test Accuracy: 0.9931\n",
            "Perplexity: 1.0754\n",
            "BLEU Score: 0.0000\n",
            "Recall: 0.0714\n",
            "Epoch 4/5\n",
            "1520/1520 [==============================] - 485s 319ms/step - loss: 0.0515 - accuracy: 0.9948 - val_loss: 0.0462 - val_accuracy: 0.9956\n",
            "317/570 [===============>..............] - ETA: 34s - loss: 0.0473 - accuracy: 0.9955"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Masking\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Enable mixed precision training\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the OOV token\n",
        "embedding_dim = 128\n",
        "latent_dim = 256\n",
        "batch_size = 48\n",
        "epochs = 5\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_input, test_input, train_output, test_output = train_test_split(input_data, output_data, test_size=0.2, random_state=42)\n",
        "\n",
        "def create_model(vocab_size, embedding_dim, latent_dim):\n",
        "    encoder_inputs = Input(shape=(max_seq_length - 1,))\n",
        "    encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
        "    encoder_masked = Masking(mask_value=0)(encoder_embedding)\n",
        "    encoder_lstm = LSTM(latent_dim, return_state=True, dtype='float32')  # Specify dtype\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_masked)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(max_seq_length - 1,))\n",
        "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
        "    decoder_masked = Masking(mask_value=0)(decoder_embedding)\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dtype='float32')  # Specify dtype\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_masked, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "    output = decoder_dense(decoder_outputs)\n",
        "\n",
        "    model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = create_model(vocab_size, embedding_dim, latent_dim)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    history = model.fit(\n",
        "        [train_input, train_output], train_output,\n",
        "        validation_data=([test_input, test_output], test_output),\n",
        "        batch_size=batch_size,\n",
        "        epochs=1\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate([test_input, test_output], test_output)\n",
        "\n",
        "    # Calculate perplexity\n",
        "    perplexity = math.exp(test_loss)\n",
        "\n",
        "    # Calculate BLEU score and recall\n",
        "    reference = [\"both are excellent technology they are helpful in many ways for the security purpose both are super\"]\n",
        "    prediction = [\"are you a fan of google or microsoft\"]\n",
        "    bleu_score = sentence_bleu([reference], prediction)\n",
        "\n",
        "    def calculate_recall(reference, prediction):\n",
        "        reference_words = set(reference.split())\n",
        "        prediction_words = set(prediction.split())\n",
        "\n",
        "        common_words = reference_words.intersection(prediction_words)\n",
        "\n",
        "        recall = len(common_words) / len(reference_words) if len(reference_words) > 0 else 0.0\n",
        "\n",
        "        return recall\n",
        "\n",
        "    recall = calculate_recall(reference[0], prediction[0])\n",
        "\n",
        "    # Print metrics after each epoch\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Perplexity: {perplexity:.4f}\")\n",
        "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# Define the encoder model\n",
        "encoder_inputs = Input(shape=(max_seq_length - 1,))\n",
        "encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_masked = Masking(mask_value=0)(encoder_embedding)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_masked)\n",
        "encoder_states = [state_h, state_c]\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "\n",
        "# Define the decoder model\n",
        "decoder_input_h = Input(shape=(latent_dim,))\n",
        "decoder_input_c = Input(shape=(latent_dim,))\n",
        "decoder_input_sequence = Input(shape=(1,))\n",
        "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_input_sequence)\n",
        "decoder_lstm_output, decoder_output_h, decoder_output_c = LSTM(latent_dim, return_sequences=True, return_state=True)(\n",
        "    decoder_embedding, initial_state=[decoder_input_h, decoder_input_c]\n",
        ")\n",
        "decoder_output_sequence = Dense(vocab_size, activation='softmax')(decoder_lstm_output)\n",
        "decoder_model = Model(\n",
        "    inputs=[decoder_input_sequence, decoder_input_h, decoder_input_c],\n",
        "    outputs=[decoder_output_sequence, decoder_output_h, decoder_output_c]\n",
        ")\n",
        "\n",
        "# Save the decoder weights\n",
        "decoder_model.save_weights('decoder_weights.h5')\n",
        "\n",
        "# Save the encoder weights\n",
        "encoder_model.save_weights('encoder_weights.h5')\n",
        "\n",
        "\n",
        "# Optionally, save the trained model\n",
        "model.save('/content/drive/MyDrive/chatbot_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNsbjSYLamIq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load your tokenizer (assuming you have it saved)\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Define max sequence length\n",
        "max_seq_length = 187  # Define the same max_seq_length as used during training\n",
        "\n",
        "# Function to preprocess input questions\n",
        "def preprocess_input(question):\n",
        "    question = question.lower()  # Convert to lowercase (you may need more preprocessing)\n",
        "    question = tokenizer.texts_to_sequences([question])\n",
        "    question = pad_sequences(question, maxlen=max_seq_length - 1, padding='post')\n",
        "    return question\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn4j8V7mbHBr"
      },
      "outputs": [],
      "source": [
        "# Load the encoder model\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(max_seq_length - 1,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_masked = tf.keras.layers.Masking(mask_value=0)(encoder_embedding)\n",
        "encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_masked)\n",
        "encoder_states = [state_h, state_c]\n",
        "encoder_model = tf.keras.models.Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "encoder_model.load_weights('encoder_weights.h5')\n",
        "\n",
        "# Load the decoder model\n",
        "decoder_input_h = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "decoder_input_c = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "decoder_input_sequence = tf.keras.layers.Input(shape=(1,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_input_sequence)\n",
        "decoder_lstm_output, decoder_output_h, decoder_output_c = tf.keras.layers.LSTM(\n",
        "    latent_dim, return_sequences=True, return_state=True\n",
        ")(decoder_embedding, initial_state=[decoder_input_h, decoder_input_c])\n",
        "decoder_output_sequence = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder_lstm_output)\n",
        "decoder_model = tf.keras.models.Model(\n",
        "    inputs=[decoder_input_sequence, decoder_input_h, decoder_input_c],\n",
        "    outputs=[decoder_output_sequence, decoder_output_h, decoder_output_c]\n",
        ")\n",
        "decoder_model.load_weights('decoder_weights.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB3FcemEbMdS"
      },
      "outputs": [],
      "source": [
        "def generate_response(input_question):\n",
        "    input_question = preprocess_input(input_question)\n",
        "\n",
        "    # Encode the input question\n",
        "    encoder_states = encoder_model.predict(input_question)\n",
        "\n",
        "    # Initialize the decoder input sequence with a start token (assuming 1 is the start token)\n",
        "    target_seq = np.array([1])\n",
        "\n",
        "    stop_condition = False\n",
        "    response = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        # Predict the next word in the sequence\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + encoder_states)\n",
        "\n",
        "        # Sample a word index from the output distribution\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Convert the index to a word\n",
        "        sampled_word = tokenizer.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        # Break the loop if the sequence is too long or if the end token is generated\n",
        "        if sampled_word == '' or len(response.split()) >= max_seq_length - 1:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            response += sampled_word + ' '\n",
        "\n",
        "        # Update the target sequence\n",
        "        target_seq = np.array([sampled_token_index])\n",
        "\n",
        "        # Update states for the next iteration\n",
        "        encoder_states = [h, c]\n",
        "\n",
        "    return response.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAIFlgaobQPc"
      },
      "outputs": [],
      "source": [
        "user_question = \"What is the weather today?\"\n",
        "response = generate_response(user_question)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "SxvgBcAHrQOW",
        "outputId": "6b0a9a8e-2ebe-425a-d9a5-05f513ff7787"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4855e399e537>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Generate and postprocess a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"How does this chatbot work?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mpostprocessed_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: generate_response() missing 5 required positional arguments: 'encoder_model', 'decoder_model', 'tokenizer', 'max_seq_length', and 'max_response_length'"
          ]
        }
      ],
      "source": [
        "def postprocess_response(generated_response):\n",
        "    # Convert the numerical sequence back to text using the vocabulary\n",
        "    generated_text = ' '.join([tokenizer.index_word[index] for index in generated_response])\n",
        "\n",
        "    # Perform postprocessing, such as capitalization and joining words\n",
        "    generated_text = generated_text.capitalize()  # Capitalize the first letter\n",
        "    generated_text = generated_text.replace(\" i \", \" I \")  # Capitalize the word \"i\"\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate and postprocess a response\n",
        "question = \"How does this chatbot work?\"\n",
        "response = generate_response(question)\n",
        "postprocessed_response = postprocess_response(response)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Generated Response:\", response)\n",
        "print(\"Postprocessed Response:\", postprocessed_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOHrZB7CtXu1"
      },
      "outputs": [],
      "source": [
        "def chatbot_interaction():\n",
        "    print(\"Chatbot: Hi! How can I help you? (Type 'exit' to end the conversation)\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        response = generate_response(user_input)\n",
        "        postprocessed_response = postprocess_response(response)\n",
        "\n",
        "        print(\"Chatbot:\", postprocessed_response)\n",
        "\n",
        "# Start the chatbot interaction\n",
        "chatbot_interaction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAtq7gYwttrJ"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def calculate_bleu_score(reference_corpus, generated_corpus):\n",
        "    bleu_score = corpus_bleu(reference_corpus, generated_corpus)\n",
        "    return bleu_score\n",
        "\n",
        "# Example reference and generated responses for evaluation\n",
        "reference_responses = [['this', 'is', 'a', 'reference', 'response']]\n",
        "generated_responses = [['this', 'is', 'a', 'generated', 'response']]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = calculate_bleu_score(reference_responses, generated_responses)\n",
        "print(f\"BLEU Score: {bleu_score:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
